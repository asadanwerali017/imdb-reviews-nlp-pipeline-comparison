{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2f459a-b2e7-45e0-be42-a9914b6c18c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (rows, columns): (50000, 2)\n",
      "\n",
      "Columns: ['review', 'sentiment']\n",
      "\n",
      "first 3 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\asad0\\OneDrive\\Documents\\Fall 2025\\Natural Language Processing\\midterm\\IMDB Dataset.csv\")\n",
    "\n",
    "print(\"Shape (rows, columns):\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nfirst 3 rows:\")\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93fd45a3-e1b5-4db6-984e-5f0520d8632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# counting how many reviews per sentiment\n",
    "sentiment_counts = df[\"sentiment\"].value_counts()\n",
    "\n",
    "print(\"sentiment distribution:\")\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2986852-a2b4-4d11-9609-83adf2aa9f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE REVIEW:\n",
      "\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fac\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NEGATIVE REVIEW: \n",
      "\n",
      "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n"
     ]
    }
   ],
   "source": [
    "# picking one positive and one negative review\n",
    "positive_review = df[df[\"sentiment\"] == \"positive\"][\"review\"].iloc[0]\n",
    "negative_review = df[df[\"sentiment\"] == \"negative\"][\"review\"].iloc[0]\n",
    "\n",
    "print(\"POSITIVE REVIEW:\\n\")\n",
    "print(positive_review[:1000]) # print first 1000 characters\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "print(\"NEGATIVE REVIEW: \\n\")\n",
    "print(negative_review[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb4e08d3-5a9e-495e-9f5a-28b692562a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CLEANING:\n",
      "\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "After Cleaning: \n",
      "\n",
      "one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me.the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word.it is called oz as that is the nickname\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_clean(text):\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove HTML tags like <br />\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# take one raw review\n",
    "raw_review = df[\"review\"].iloc[0]\n",
    "\n",
    "cleaned_review = basic_clean(raw_review)\n",
    "\n",
    "print(\"BEFORE CLEANING:\\n\")\n",
    "print(raw_review[:500])\n",
    "\n",
    "print(\"\\n\" +\"-\"*80 + \"\\n\")\n",
    "\n",
    "print(\"After Cleaning: \\n\")\n",
    "print(cleaned_review[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "645c6ff9-077d-4ecb-ad28-095a4567dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: \n",
      "\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "AFTER:\n",
      "\n",
      "reviewers mentioned watching just 1 oz episode youll hooked right exactly happened mei say main appeal fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess episode saw struck nasty surreal say ready watched developed taste oz got accustomed high levels graphic violence not just violence injustice crooked guards wholl sold nickel\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# customize stopwords: remove negation words from stopword list\n",
    "custom_stopwords = ENGLISH_STOP_WORDS - {\"not\", \"no\", \"never\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove HTML\n",
    "    text = re.sub(r\"<.*>\", \"\", text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "    # remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in custom_stopwords]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# test on one review\n",
    "raw_review = df[\"review\"].iloc[0]\n",
    "cleaned_review = clean_text(raw_review)\n",
    "\n",
    "print(\"BEFORE: \\n\")\n",
    "print(raw_review[:400])\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "print(\"AFTER:\\n\")\n",
    "print(cleaned_review[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bcd83b0-13ed-4b86-9ad1-f8e1f2fc32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['accustomed' 'appeal' 'audiences' 'away' 'bitches' 'charm' 'class'\n",
      " 'comes' 'comfortable' 'concerning' 'crooked' 'dare' 'darker' 'decorating'\n",
      " 'developed' 'disappears' 'doesnt' 'dream' 'episode' 'exactly'\n",
      " 'experience' 'fact' 'fantasy' 'flat' 'forget' 'goes' 'got' 'graphic'\n",
      " 'guard' 'guards' 'halliwell' 'halliwells' 'happened' 'high' 'home'\n",
      " 'hooked' 'injustice' 'inmates' 'just' 'kill' 'knowledge' 'lack' 'levels'\n",
      " 'little' 'main' 'mainstream' 'mannered' 'mei' 'mentioned' 'mess' 'middle'\n",
      " 'murals' 'nasty' 'nickel' 'not' 'order' 'orton' 'oz' 'painted'\n",
      " 'particularly' 'pictures' 'plays' 'pretty' 'prison' 'production' 'ready'\n",
      " 'realism' 'really' 'remains' 'reviewers' 'right' 'romanceoz' 'saw' 'say'\n",
      " 'scenes' 'senses' 'sets' 'shows' 'skills' 'sold' 'solid' 'street'\n",
      " 'struck' 'surface' 'surreal' 'taste' 'techniques' 'terribly' 'things'\n",
      " 'touch' 'traditional' 'turned' 'uncomfortable' 'use' 'viewingthats'\n",
      " 'violence' 'watched' 'watching' 'wholl' 'wonderful' 'wouldnt' 'youll']\n",
      "\n",
      "Numerical reprsentation (Bag of Words):\n",
      "[[1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 2 1 1 1 0 0 3 1 1 1 0 1 0 0 1 1 0 1\n",
      "  1 2 2 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 3 1 0 1 0 1 2 0 1 0 0 0 1 1 1\n",
      "  1 2 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 2 1 2 2 0 1 1]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
      "  0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 2 0 1 0 0 1 0 1 1 1 0 0 0\n",
      "  0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# take 2 cleaned reviews \n",
    "reviews = [\n",
    "    clean_text(df[\"review\"].iloc[0]),\n",
    "    clean_text(df[\"review\"].iloc[1])\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nNumerical reprsentation (Bag of Words):\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2aaa4cd-54f3-40ff-a3a8-208102e4cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary:\n",
      "['accustomed' 'appeal' 'audiences' 'away' 'bitches' 'charm' 'class'\n",
      " 'comes' 'comfortable' 'concerning' 'crooked' 'dare' 'darker' 'decorating'\n",
      " 'developed' 'disappears' 'doesnt' 'dream' 'episode' 'exactly'\n",
      " 'experience' 'fact' 'fantasy' 'flat' 'forget' 'goes' 'got' 'graphic'\n",
      " 'guard' 'guards' 'halliwell' 'halliwells' 'happened' 'high' 'home'\n",
      " 'hooked' 'injustice' 'inmates' 'just' 'kill' 'knowledge' 'lack' 'levels'\n",
      " 'little' 'main' 'mainstream' 'mannered' 'mei' 'mentioned' 'mess' 'middle'\n",
      " 'murals' 'nasty' 'nickel' 'not' 'order' 'orton' 'oz' 'painted'\n",
      " 'particularly' 'pictures' 'plays' 'pretty' 'prison' 'production' 'ready'\n",
      " 'realism' 'really' 'remains' 'reviewers' 'right' 'romanceoz' 'saw' 'say'\n",
      " 'scenes' 'senses' 'sets' 'shows' 'skills' 'sold' 'solid' 'street'\n",
      " 'struck' 'surface' 'surreal' 'taste' 'techniques' 'terribly' 'things'\n",
      " 'touch' 'traditional' 'turned' 'uncomfortable' 'use' 'viewingthats'\n",
      " 'violence' 'watched' 'watching' 'wholl' 'wonderful' 'wouldnt' 'youll']\n",
      "\n",
      "TF-IDF numerical representation:\n",
      "[[0.09534626 0.09534626 0.09534626 0.09534626 0.09534626 0.09534626\n",
      "  0.09534626 0.         0.09534626 0.         0.09534626 0.09534626\n",
      "  0.09534626 0.         0.09534626 0.         0.09534626 0.\n",
      "  0.19069252 0.09534626 0.09534626 0.09534626 0.         0.\n",
      "  0.28603878 0.09534626 0.09534626 0.09534626 0.         0.09534626\n",
      "  0.         0.         0.09534626 0.09534626 0.         0.09534626\n",
      "  0.09534626 0.19069252 0.19069252 0.09534626 0.         0.09534626\n",
      "  0.09534626 0.         0.09534626 0.09534626 0.09534626 0.09534626\n",
      "  0.09534626 0.09534626 0.09534626 0.         0.09534626 0.09534626\n",
      "  0.09534626 0.09534626 0.         0.28603878 0.09534626 0.\n",
      "  0.09534626 0.         0.09534626 0.19069252 0.         0.09534626\n",
      "  0.         0.         0.         0.09534626 0.09534626 0.09534626\n",
      "  0.09534626 0.19069252 0.         0.         0.         0.09534626\n",
      "  0.09534626 0.09534626 0.         0.09534626 0.09534626 0.\n",
      "  0.09534626 0.09534626 0.         0.         0.         0.09534626\n",
      "  0.         0.09534626 0.09534626 0.         0.09534626 0.19069252\n",
      "  0.09534626 0.19069252 0.19069252 0.         0.09534626 0.09534626]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.16222142 0.         0.16222142 0.         0.\n",
      "  0.         0.16222142 0.         0.16222142 0.         0.16222142\n",
      "  0.         0.         0.         0.         0.16222142 0.16222142\n",
      "  0.         0.         0.         0.         0.16222142 0.\n",
      "  0.16222142 0.16222142 0.         0.         0.16222142 0.\n",
      "  0.         0.         0.         0.         0.16222142 0.\n",
      "  0.         0.32444284 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.16222142 0.         0.\n",
      "  0.         0.         0.16222142 0.         0.         0.32444284\n",
      "  0.         0.16222142 0.         0.         0.16222142 0.\n",
      "  0.16222142 0.16222142 0.16222142 0.         0.         0.\n",
      "  0.         0.         0.16222142 0.16222142 0.16222142 0.\n",
      "  0.         0.         0.16222142 0.         0.         0.16222142\n",
      "  0.         0.         0.16222142 0.16222142 0.16222142 0.\n",
      "  0.16222142 0.         0.         0.16222142 0.         0.\n",
      "  0.         0.         0.         0.16222142 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "print(\"TF-IDF Vocabulary:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF numerical representation:\")\n",
    "print(X_tfidf.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39f58c3c-caf6-4a70-8e9a-81797d9393b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1) X = raw text raw text - I will vectorize it. \n",
    "X_text = df[\"review\"]\n",
    "\n",
    "# 2) y = labels (positive/negative)\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "# 3) Train/test split (random_state=42 for reproducibility\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 4) TF_IDF vectorization (fit on train, transform train+test)\n",
    "tfidf = TfidfVectorizer(preprocessor=clean_text)\n",
    "X_train = tfidf.fit_transform(X_train_text)\n",
    "X_test = tfidf.transform(X_test_text)\n",
    "\n",
    "# 5) train a logistic regression classifier\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Quick accuracy check\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab3e7bf7-9091-49ee-af0e-d01ea700628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.84      0.85      5000\n",
      "    positive       0.85      0.87      0.86      5000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      "[[4212  788]\n",
      " [ 673 4327]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1) Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 2) Print precision, recall, F1-score\n",
    "print(\"Classification Report: \\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 3 Print Confusion Matrix\n",
    "print(\"Confusion Matrix: \\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6427997a-a782-433b-bfc3-e34b35c42700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model Accuracy: 0.8599\n",
      "\n",
      "Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.84      0.86      5000\n",
      "    positive       0.85      0.88      0.86      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Confusion Matrix: \n",
      "\n",
      "[[4221  779]\n",
      " [ 622 4378]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# TF-IDF with Unigrams +  Bigrams\n",
    "tfidf_bigram = TfidfVectorizer(\n",
    "    preprocessor=clean_text,\n",
    "    ngram_range=(1,2), # (1,2) = unigrams + bigrams\n",
    "    min_df=2  # ignore words/phrases that appear in only one document\n",
    ")\n",
    "\n",
    "X_train_bigram = tfidf_bigram.fit_transform(X_train_text)\n",
    "X_test_bigram = tfidf_bigram.transform(X_test_text)\n",
    "\n",
    "model_bigram = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_bigram.fit(X_train_bigram, y_train)\n",
    "\n",
    "y_pred_bigram = model_bigram.predict(X_test_bigram)\n",
    "\n",
    "print(\"Bigram Model Accuracy:\", model_bigram.score(X_test_bigram, y_test))\n",
    "print(\"\\nClassification Report: \\n\")\n",
    "print(classification_report(y_test, y_pred_bigram))\n",
    "print(\"Confusion Matrix: \\n\")\n",
    "print(confusion_matrix(y_test, y_pred_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc6f330f-b89d-48bf-aea2-c78f18557254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33553    really liked summerslam look arena curtains ju...\n",
       "9427     not television shows appeal quite different ki...\n",
       "199      film quickly gets major chase scene increasing...\n",
       "12447    jane austen definitely approve onehighly recom...\n",
       "39489    expectations somewhat high went movie thought ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a subset for topic modelling (for speed and clarity)\n",
    "topic_texts = df[\"review\"].sample(n=5000, random_state=42)\n",
    "\n",
    "# apply cleaning\n",
    "topic_texts_cleaned = topic_texts.apply(clean_text)\n",
    "\n",
    "# Quick sanity check\n",
    "topic_texts_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ecb4b9b-d545-4e53-bee5-399b6fa590b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "['film', 'not', 'movie', 'just', 'time', 'no', 'like', 'dont', 'good', 'really']\n",
      "\n",
      "Topic 2:\n",
      "['movie', 'not', 'film', 'just', 'like', 'no', 'horror', 'good', 'watch', 'really']\n",
      "\n",
      "Topic 3:\n",
      "['movie', 'not', 'like', 'just', 'bad', 'good', 'seen', 'time', 'movies', 'really']\n",
      "\n",
      "Topic 4:\n",
      "['movie', 'not', 'good', 'just', 'like', 'great', 'really', 'film', 'story', 'bad']\n",
      "\n",
      "Topic 5:\n",
      "['film', 'not', 'films', 'story', 'best', 'movie', 'like', 'life', 'great', 'good']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 1) Convert text to word counts\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.9,   # ignore very common words\n",
    "    min_df=10,    # ignore very rare words\n",
    ")\n",
    "\n",
    "X_counts = count_vectorizer.fit_transform(topic_texts_cleaned)\n",
    "\n",
    "# 2) Train LDA model\n",
    "num_topics = 5\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=num_topics, \n",
    "    random_state=42,\n",
    "    learning_method=\"batch\"\n",
    "\n",
    ") \n",
    "\n",
    "lda_model.fit(X_counts)\n",
    "\n",
    "# 3) Function to display top words per topic\n",
    "def display_topics(model, feature_names, num_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        top_words = topic.argsort()[-num_top_words:][::-1]\n",
    "        print([feature_names[i] for i in top_words])\n",
    "\n",
    "# 4) Show topics\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "display_topics(lda_model, feature_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8576a37c-1099-4c76-ac49-00663595586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Extended stopwords for topic modeling\n",
    "topic_stopwords = ENGLISH_STOP_WORDS.union({\n",
    "    \"movie\", \"film\", \"films\", \"movie\",\n",
    "    \"good\", \"bad\", \"great\", \"really\", \"just\",\n",
    "    \"like\", \"dont\", \"didnt\", \"doesnt\", \"not\", \n",
    "    \"watch\", \"watched\", \"watching\", \"seen\", \n",
    "    \"time\", \"story\"\n",
    "})\n",
    "\n",
    "def clean_text_topic(text):\n",
    "    text =  text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text) # keep letters only\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in topic_stopwords and len(w) > 2 ]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c008660-0b3a-42c2-85a5-c275a7566053",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_texts_cleaned = topic_texts.apply(clean_text_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fb1819a-a0dc-440b-94a2-4198fc8eb4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "['movies', 'make', 'acting', 'people', 'horror', 'think', 'plot', 'effects', 'characters', 'scene']\n",
      "\n",
      "Topic 2:\n",
      "['characters', 'character', 'series', 'plot', 'original', 'does', 'episode', 'acting', 'new', 'game']\n",
      "\n",
      "Topic 3:\n",
      "['love', 'best', 'comedy', 'funny', 'years', 'old', 'role', 'man', 'young', 'cast']\n",
      "\n",
      "Topic 4:\n",
      "['life', 'people', 'world', 'love', 'war', 'young', 'american', 'way', 'best', 'man']\n",
      "\n",
      "Topic 5:\n",
      "['horror', 'man', 'little', 'end', 'way', 'scene', 'killer', 'gets', 'does', 'character']\n",
      "\n",
      "Topic 6:\n",
      "['people', 'movies', 'think', 'know', 'did', 'say', 'better', 'make', 'acting', 'thing']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    min_df=20,     # stronger filtering\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "X_counts = count_vectorizer.fit_transform(topic_texts_cleaned)\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=6,\n",
    "    random_state=42,\n",
    "    learning_method=\"batch\"\n",
    ")\n",
    "\n",
    "lda_model.fit(X_counts)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "display_topics(lda_model, feature_names, num_top_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5cb4cdec-bd83-4efd-84f9-eb33b4495bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "['man', 'character', 'role', 'scene', 'does', 'best', 'performance', 'young', 'characters', 'director']\n",
      "\n",
      "Topic 2:\n",
      "['acting', 'plot', 'worst', 'better', 'movies', 'make', 'people', 'did', 'say', 'know']\n",
      "\n",
      "Topic 3:\n",
      "['love', 'life', 'people', 'think', 'saw', 'did', 'family', 'book', 'years', 'loved']\n",
      "\n",
      "Topic 4:\n",
      "['funny', 'comedy', 'laugh', 'jokes', 'humor', 'hilarious', 'best', 'fun', 'funniest', 'lot']\n",
      "\n",
      "Topic 5:\n",
      "['horror', 'movies', 'gore', 'little', 'dead', 'scary', 'fun', 'house', 'killer', 'effects']\n",
      "\n",
      "Topic 6:\n",
      "['series', 'episode', 'characters', 'episodes', 'season', 'new', 'original', 'television', 'character', 'shows']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asad0\\OneDrive\\Documents\\PYTHON ANNACONDO\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# TF-IDF for topic modeling\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=20,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "X_tfidf_topics = tfidf_vectorizer.fit_transform(topic_texts_cleaned)\n",
    "\n",
    "# Train NMF\n",
    "num_topics = 6\n",
    "\n",
    "nmf_model = NMF(\n",
    "    n_components=num_topics,\n",
    "    random_state=42,\n",
    "    init=\"nndsvd\"\n",
    ")\n",
    "\n",
    "nmf_model.fit(X_tfidf_topics)\n",
    "\n",
    "# display topics\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def display_topics_nmf(model, feature_names, num_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        top_words =  topic.argsort()[-num_top_words:][::-1]\n",
    "        print([feature_names[i] for i in top_words])\n",
    "\n",
    "display_topics_nmf(nmf_model, feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b330f33a-dad2-451a-82b9-ace0bfc34aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "['man', 'character', 'role', 'scene', 'does', 'best', 'performance', 'young', 'characters', 'director']\n",
      "\n",
      "Topic 2:\n",
      "['acting', 'plot', 'worst', 'better', 'movies', 'make', 'did', 'people', 'say', 'know']\n",
      "\n",
      "Topic 3:\n",
      "['love', 'life', 'people', 'think', 'saw', 'did', 'family', 'book', 'years', 'loved']\n",
      "\n",
      "Topic 4:\n",
      "['funny', 'comedy', 'laugh', 'jokes', 'humor', 'hilarious', 'best', 'fun', 'funniest', 'lot']\n",
      "\n",
      "Topic 5:\n",
      "['horror', 'movies', 'gore', 'little', 'dead', 'scary', 'fun', 'house', 'killer', 'effects']\n",
      "\n",
      "Topic 6:\n",
      "['series', 'episode', 'characters', 'episodes', 'season', 'new', 'original', 'television', 'character', 'shows']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# TF-IDF for topic modeling\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=20,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "X_tfidf_topics = tfidf_vectorizer.fit_transform(topic_texts_cleaned)\n",
    "\n",
    "# Train NMF\n",
    "num_topics = 6\n",
    "\n",
    "nmf_model = NMF(\n",
    "    n_components=num_topics,\n",
    "    random_state=42,\n",
    "    init=\"nndsvd\",\n",
    "    max_iter=500  # increased from default 200\n",
    ")\n",
    "\n",
    "nmf_model.fit(X_tfidf_topics)\n",
    "\n",
    "# display topics\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def display_topics_nmf(model, feature_names, num_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        top_words =  topic.argsort()[-num_top_words:][::-1]\n",
    "        print([feature_names[i] for i in top_words])\n",
    "\n",
    "display_topics_nmf(nmf_model, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0eac785c-8c68-4829-8206-b2a1bbf440bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#... 1) Base cleaning\n",
    "# we will define variants below and swap them in\n",
    "\n",
    "def clean_base(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \".\", text)          # removing HTML\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)       # keep letters / spaces only\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "    \n",
    "#... 2) Variant: base + remove stopwords (but keep \"not\")\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "STOPWORDS_KEEP_NOT = set(ENGLISH_STOP_WORDS) - {\"not\", \"no\", \"nor\"}\n",
    "\n",
    "def clean_no_stopwords_keep_not(text: str) -> str:\n",
    "    text = clean_base(text)\n",
    "    words = [w for w in text.split() if w not in STOPWORDS_KEEP_NOT]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "#.... 3 Train / evaluate helper ---\n",
    "def run_sentiment_pipeline(clean_fn, ngram_range=(1,1), min_df=2, max_df=0.9):\n",
    "    X_text = df[\"review\"]\n",
    "    y = df[\"sentiment\"]\n",
    "\n",
    "\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        preprocessor=clean_fn,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df\n",
    "    )\n",
    "\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train_text)\n",
    "    X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = model.score(X_test, y_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    return acc, cm, report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "78f1c88a-7c0c-47c1-9d2c-b49895307d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1 (base cleaning, unigrams) accuracy: 0.9005\n",
      "Confusion matrix:\n",
      " [[4481  519]\n",
      " [ 476 4524]]\n"
     ]
    }
   ],
   "source": [
    "# pipeline 1 Stopwords kept, Unigrams\n",
    "\n",
    "acc1, cm1, rep1 = run_sentiment_pipeline(clean_base, ngram_range=(1,1))\n",
    "print(\"Pipeline 1 (base cleaning, unigrams) accuracy:\", acc1)\n",
    "print(\"Confusion matrix:\\n\", cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "848639af-5d2a-4fa7-b9f2-092318b23480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 2 (base cleaning, unigrams+bigrams) accuracy: 0.9062\n",
      "Confusion matrix:\n",
      " [[4494  506]\n",
      " [ 432 4568]]\n"
     ]
    }
   ],
   "source": [
    "# pipeline 2, base cleaning, bigrams\n",
    "\n",
    "acc2, cm2, rep2 = run_sentiment_pipeline(clean_base, ngram_range=(1,2))\n",
    "print(\"Pipeline 2 (base cleaning, unigrams+bigrams) accuracy:\", acc2)\n",
    "print(\"Confusion matrix:\\n\", cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62668d4f-a2d8-4152-8de2-dc1efc9c5092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 3 (no stopwords but keep NOT, unigrams+bigrams) accuracy: 0.9007\n",
      "Confusion matrix:\n",
      " [[4455  545]\n",
      " [ 448 4552]]\n"
     ]
    }
   ],
   "source": [
    "# pipeline 3, remove stopwords, keep NOT, bigrams\n",
    "\n",
    "acc3, cm3, rep3 = run_sentiment_pipeline(clean_no_stopwords_keep_not, ngram_range=(1,2))\n",
    "print(\"Pipeline 3 (no stopwords but keep NOT, unigrams+bigrams) accuracy:\", acc3)\n",
    "print(\"Confusion matrix:\\n\", cm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c12dc742-247b-4b04-ba55-405fde577fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy is installed and en_core_web_sm is loaded\n"
     ]
    }
   ],
   "source": [
    "# ensuring SpaCy + English model are available\n",
    "\n",
    "try: \n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"spacy is installed and en_core_web_sm is loaded\")\n",
    "    except OSError:\n",
    "        print(\"SpaCy is installed but en_core_web_sm model is missing. Installing model...\")\n",
    "        import sys\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"en_core_web_sm installed and loaded.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"SpaCy is not installed. Installing SpaCy + model...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"spacy\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy + en_core_web_sm installed and loaded.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f17d8932-3992-44f2-86a0-7d41bcf22f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 4 (spaCy lemmatization + TF-IDF bigrams) Accuracy: 0.8964\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.88      0.89      5000\n",
      "    positive       0.89      0.91      0.90      5000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[4409  591]\n",
      " [ 445 4555]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Keep negations (important for sentiment)\n",
    "STOPWORDS_KEEP_NOT = set(ENGLISH_STOP_WORDS) - {\"not\", \"no\", \"nor\"}\n",
    "\n",
    "def clean_lemmatize(text: str) -> str:\n",
    "    # 1) lowercase + remove HTML\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "\n",
    "    # 2) spaCy lemmatization\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # 3) keep useful lemmas only\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        # skip spaces/punct/numbers\n",
    "        if tok.is_space or tok.is_punct or tok.like_num:\n",
    "            continue\n",
    "\n",
    "        lemma = tok.lemma_.strip()\n",
    "\n",
    "        # remove very short tokens and stopwords (but keep not/no/nor)\n",
    "        if len(lemma) <= 2:\n",
    "            continue\n",
    "        if lemma in STOPWORDS_KEEP_NOT:\n",
    "            continue\n",
    "\n",
    "        tokens.append(lemma)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Vectorize with TF-IDF (unigrams + bigrams)\n",
    "tfidf_lemma = TfidfVectorizer(\n",
    "    preprocessor=clean_lemmatize,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "X_train_lemma = tfidf_lemma.fit_transform(X_train_text)\n",
    "X_test_lemma = tfidf_lemma.transform(X_test_text)\n",
    "\n",
    "model_lemma = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_lemma.fit(X_train_lemma, y_train)\n",
    "\n",
    "y_pred_lemma = model_lemma.predict(X_test_lemma)\n",
    "\n",
    "print(\"Pipeline 4 (spaCy lemmatization + TF-IDF bigrams) Accuracy:\", model_lemma.score(X_test_lemma, y_test))\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_lemma))\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb01cfc-a7ae-4d4d-a6ad-f10f5cc1ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
